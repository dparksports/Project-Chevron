======================================================================
Γù¼  SCP System Prompt: Transcriber
   Size: 5571 chars (~815 tokens)
======================================================================

Γù¼ ΓöÇΓöÇΓöÇ Sending to gemini-3-pro-preview: Implement Transcriber ΓöÇΓöÇΓöÇ

```python
import os
import logging
import sys
from typing import List, Optional, NamedTuple, Union
from dataclasses import dataclass

# Standard library imports are allowed.
# Third-party imports for core functionality.
try:
    import torch
    import numpy as np
    from faster_whisper import WhisperModel
except ImportError:
    # In a real environment, these are required dependencies.
    # For the purpose of code generation, we proceed assuming they exist.
    WhisperModel = None
    np = None
    torch = None

# Visible Dependency Interfaces
# We import these to access their public interfaces as permitted.
try:
    from audio_ingest import AudioIngest
    from voice_detector import VoiceDetector
except ImportError:
    # These modules are part of the project environment.
    pass

# Setup module-level logger
logger = logging.getLogger(__name__)

# -----------------------------------------------------------------------------
# Data Structures
# -----------------------------------------------------------------------------

@dataclass
class TranscriptSegment:
    """Represents a single segment of transcribed text with timing."""
    start: float
    end: float
    text: str

@dataclass
class Transcript:
    """Represents the full transcription of a file."""
    file_path: str
    segments: List[TranscriptSegment]
    full_text: str
    model_name: str

# -----------------------------------------------------------------------------
# Module State
# -----------------------------------------------------------------------------

# Singleton cache for the model to satisfy the "Origin" constraint of loading once.
_MODEL_CACHE: Optional['WhisperModel'] = None
_MODEL_NAME_CACHE: Optional[str] = None

# -----------------------------------------------------------------------------
# Glyph: Γù¼ The Origin
# -----------------------------------------------------------------------------

def load_model(model_name: str, device: str, compute: str) -> 'WhisperModel':
    """
    Γù¼ Origin ΓÇö loads the Whisper model with CPU fallback.
    
    Contract: Accepts initial data ΓåÆ Produces a data stream (Model Instance).
    Constraint: Must appear exactly once per program (Logical Singleton).
    """
    global _MODEL_CACHE, _MODEL_NAME_CACHE

    # Return cached model if available and matching
    if _MODEL_CACHE is not None and _MODEL_NAME_CACHE == model_name:
        return _MODEL_CACHE

    print(f"[PROGRESS] Initializing Whisper Model: {model_name} on {device} ({compute})...")

    try:
        # Attempt to load on the requested device (usually GPU)
        model = WhisperModel(model_name, device=device, compute_type=compute)
        print(f"[PROGRESS] Model {model_name} loaded successfully on {device}.")
    except Exception as e:
        print(f"[PROGRESS] Failed to load model on {device}: {e}")
        print("[PROGRESS] Attempting graceful fallback to CPU...")
        
        try:
            # Fallback to CPU with int8 quantization for compatibility
            model = WhisperModel(model_name, device="cpu", compute_type="int8")
            print(f"[PROGRESS] Model {model_name} loaded successfully on CPU.")
        except Exception as cpu_e:
            print(f"[PROGRESS] Critical Failure: Could not load model on CPU. {cpu_e}")
            raise cpu_e

    # Update cache
    _MODEL_CACHE = model
    _MODEL_NAME_CACHE = model_name
    
    return model

# -----------------------------------------------------------------------------
# Glyph: Γÿ╛ Fold Time
# -----------------------------------------------------------------------------

def transcribe_file(file_path: str, model_name: str, beam_size: int) -> Transcript:
    """
    Γÿ╛ Fold Time ΓÇö Folds audio through model ΓÇö iterates segments into full transcript.
    
    Contract: Accepts (predicate, transform, value) ΓåÆ Produces final value.
    Constraint: Must always have a reachable base case. Must not mutate external state.
    """
    # 1. Determine Output Path
    directory = os.path.dirname(file_path)
    filename = os.path.basename(file_path)
    base_name = os.path.splitext(filename)[0]
    transcript_filename = f"{base_name}_transcript_{model_name}.txt"
    output_path = os.path.join(directory, transcript_filename)

    # 2. Check Base Case (Skip Existing)
    if os.path.exists(output_path):
        print(f"[PROGRESS] Skipping existing transcript: {transcript_filename}")
        try:
            with open(output_path, 'r', encoding='utf-8') as f:
                full_text = f.read()
            # Return reconstructed object (segments lost in plain text, returning empty list)
            return Transcript(file_path, [], full_text, model_name)
        except IOError:
            print(f"[PROGRESS] Error reading existing file, re-transcribing: {file_path}")

    # 3. Prepare Environment (Device Config)
    try:
        # Call Interface: AudioIngest
        device_config = AudioIngest.get_device_config(None)
        device = device_config.device
        compute = device_config.compute_type
    except (NameError, AttributeError):
        # Fallback if interface is unreachable
        device = "cuda" if torch and torch.cuda.is_available() else "cpu"
        compute = "float16" if device == "cuda" else "int8"

    # 4. Load Model (Invoke Origin)
    model = load_model(model_name, device, compute)

    # 5. Perform Transcription (The Fold)
    print(f"[PROGRESS] Transcribing file: {filename}")
    try:
        # faster-whisper returns a generator
        segments_gen, info = model.transcribe(file_path, beam_size=beam_size)
        
        segments_list = []
        text_parts = []

        for segment in segments_gen:
            # Transform stream into data structure
            seg_obj = TranscriptSegment(
                start=segment.start,
                end=segment.end,
                text=segment.text.strip()
            )
            segments_list.append(seg_obj)
            text_parts.append(segment.text.strip())
            
            # Optional: Emit progress for long files
            # print(f"[PROGRESS] Processed segment {segment.start:.1f}s - {segment.end:.1f}s")

        full_text = " ".join(text_parts)

        # 6. Save Artifact (Constraint)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(full_text)
        print(f"[PROGRESS] Saved transcript: {transcript_filename}")

        return Transcript(file_path, segments_list, full_text, model_name)

    except Exception as e:
        print(f"[PROGRESS] Error transcribing {filename}: {e}")
        # Return empty transcript on failure to maintain type safety
        return Transcript(file_path, [], "", model_name)

def batch_transcribe(directory: str, model_name: str, beam_size: int) -> List[Transcript]:
    """
    Γÿ╛ Fold Time ΓÇö Recursively folds all files in directory through transcription.
    
    Contract: Accepts (predicate, transform, value) ΓåÆ Produces final value.
    """
    print(f"[PROGRESS] Starting batch transcription in: {directory}")
    
    # 1. Discovery (Interface Call)
    try:
        media_files = AudioIngest.find_media(directory)
    except NameError:
        print("[PROGRESS] AudioIngest interface unavailable.")
        return []

    results = []

    # 2. Recursion/Iteration (The Fold)
    for media in media_files:
        # Extract path from MediaFile interface
        # Assuming MediaFile has 'path' attribute or is a path string
        file_path = getattr(media, 'path', str(media))
        
        # Apply Transformation
        transcript = transcribe_file(file_path, model_name, beam_size)
        results.append(transcript)

    print(f"[PROGRESS] Batch transcription complete. Processed {len(results)} files.")
    return results

def transcribe_segment(file_path: str, start: float, end: float) -> Transcript:
    """
    Γÿ╛ Fold Time ΓÇö Folds a specific time range through the model.
    
    Contract: Accepts (predicate, transform, value) ΓåÆ Produces final value.
    """
    # 1. Resolve Model
    # This method signature lacks model_name, so we use the cached one or default.
    global _MODEL_CACHE, _MODEL_NAME_CACHE
    if _MODEL_CACHE is None:
        # Default fallback if no model loaded yet
        load_model("base", "cpu", "int8")
    
    model = _MODEL_CACHE
    model_name = _MODEL_NAME_CACHE if _MODEL_NAME_CACHE else "unknown"

    # 2. Load and Slice Audio (Interface Call)
    try:
        # AudioIngest.load_audio returns AudioTensor
        audio_tensor = AudioIngest.load_audio(file_path)
        
        # Handle Tensor Slicing
        # Assuming 16kHz sample rate (Whisper standard)
        SR = 16000
        start_idx = int(start * SR)
        end_idx = int(end * SR)
        
        # Convert to numpy if it's a torch tensor
        if hasattr(audio_tensor, 'cpu'):
            audio_data = audio_tensor.cpu().numpy()
        else:
            audio_data = audio_tensor

        # Slice dimensions
        if hasattr(audio_data, 'ndim') and audio_data.ndim > 1:
            # Take first channel if stereo
            audio_slice = audio_data[0, start_idx:end_idx]
        else:
            audio_slice = audio_data[start_idx:end_idx]
            
    except Exception as e:
        print(f"[PROGRESS] Error preparing audio segment: {e}")
        return Transcript(file_path, [], "", model_name)

    # 3. Transcribe Slice
    try:
        segments_gen, info = model.transcribe(audio_slice, beam_size=5)
        
        segments_list = []
        text_parts = []
        
        for segment in segments_gen:
            # Adjust timestamps to be relative to the original file
            abs_start = start + segment.start
            abs_end = start + segment.end
            
            seg_obj = TranscriptSegment(abs_start, abs_end, segment.text.strip())
            segments_list.append(seg_obj)
            text_parts.append(segment.text.strip())
            
        full_text = " ".join(text_parts)
        
        return Transcript(file_path, segments_list, full_text, model_name)

    except Exception as e:
        print(f"[PROGRESS] Error transcribing segment: {e}")
        return Transcript(file_path, [], "", model_name)
```

Γÿñ ΓöÇΓöÇΓöÇ Weaver Verification: Transcriber ΓöÇΓöÇΓöÇ

PASS

The code conforms to the SCP architecture specification.

**Verification Report:**
1.  **Interface Conformance:** All methods (`load_model`, `transcribe_file`, `batch_transcribe`, `transcribe_segment`) match the required signatures and contracts.
2.  **Dependency Isolation:** Only permitted project modules (`AudioIngest`, `VoiceDetector`) and standard/core libraries are imported.
3.  **Constraint Compliance:**
    *   Transcripts are saved with the correct naming convention.
    *   `skip_existing` logic is implemented.
    *   Model loading implements the required fallback (GPU ΓåÆ CPU) as described in the docstring.
    *   `[PROGRESS]` markers are emitted.
    *   No analysis logic is present.
4.  **Coupling Detection:**
    *   Global state (`_MODEL_CACHE`) is restricted to the implementation of the **Origin** (Singleton) pattern for the model resource, which is permitted by the Glyph contract.
    *   Inter-module communication uses the `AudioIngest` interface explicitly.
    *   Missing dependencies are handled gracefully via `try/except` blocks, ensuring no hard crashes.
5.  **Glyph Contracts:**
    *   `load_model` (Γù¼) acts as the logical singleton origin.
    *   `transcribe_file` (Γÿ╛) folds audio to text with a base case (existing file).
    *   `batch_transcribe` (Γÿ╛) folds over a directory.
    *   `transcribe_segment` (Γÿ╛) folds over a time slice.
