# TurboScribe Ã— Chevron â€” SCP Case Study

> How the Spatial Constraint Protocol reduced a 110,000-token codebase to
> ~660 tokens per AI prompt â€” while **eliminating hallucination-driven
> regressions** that plague traditional AI code generation.

---

## The Problem

TurboScribe is a GPU-accelerated audio transcription and meeting detection
application (~7,500 LOC across Python + C#/WPF). Its Python backend
(`fast_engine.py`) is a 1,520-line monolith handling 10 distinct modes:
VAD scanning, batch transcription, semantic search, meeting detection, LLM
analysis, and timestamp extraction.

When you ask an AI to modify this codebase, it faces a fundamental challenge:

| Approach | Tokens In | Risk |
|---|---:|---|
| Paste entire codebase | **~110,000** | Exceeds most context windows; AI loses focus |
| Paste one file | **~16,000** | AI sees internal details of unrelated modes, hallucinates cross-dependencies |
| Describe changes in English | **~500** | AI invents its own contracts, breaking existing interfaces |

Every approach leads to the same failure: **AI-generated code that silently
couples modules that were previously independent**, introducing regressions
that only surface at runtime.

## The SCP Solution

The Spatial Constraint Protocol (SCP) replaces "paste everything and hope"
with **deterministic architectural contracts**. Instead of showing the AI
your code, you show it:

1. **What this module does** (interface contract)
2. **What it may depend on** (dependency list â€” interfaces only, not implementations)
3. **What it must never touch** (forbidden zones via RAG Denial)
4. **How each method behaves** (glyph semantics â€” Origin, Filter, Fold, Witness, Weaver)

The AI never sees any other module's implementation. It **cannot** hallucinate
cross-module coupling because the coupling surfaces are simply not in context.

## TurboScribe Decomposition

The monolithic `fast_engine.py` was decomposed into 9 isolated SCP modules:

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ AudioIngest  â”‚ â—¬ Origin â€” discovers & loads media
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”˜
         â”‚   â”‚
    â”Œâ”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼                                    â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ VoiceDetector â”‚ Ó¨ Filter     â”‚ TimestampExtractorâ”‚ â˜¾ Fold
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Transcriber  â”‚ â˜¾ Fold Time
  â””â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”¬â”€â”€â”˜
     â”‚     â”‚  â”‚
     â–¼     â”‚  â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚Searchâ”‚ â”‚ â”‚ MeetingDetector  â”‚ Ó¨ Filter
  â”‚Engineâ”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â””â”€â”€â”€â”€â”€â”€â”˜ â”‚          â”‚
     Ó¨     â”‚          â–¼
           â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  â”‚ LLMProvider  â”‚ â˜¤ Weaver
           â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚         â”‚
           â–¼         â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Analyzer    â”‚ â˜¤ Weaver
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘ ProgressWitness ğ“‚€ â•‘  (observes all â€” modifies nothing)
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Module Summary

| Module | Glyph | Role | Methods | Key Constraint |
|---|:---:|---|:---:|---|
| **AudioIngest** | â—¬ | File discovery & audio loading | 3 | No ML imports â€” raw I/O only |
| **VoiceDetector** | Ó¨ | Speech/silence segmentation | 3 | No transcription â€” detection only |
| **Transcriber** | â˜¾ | Whisper batch transcription | 4 | No analysis or search |
| **SearchEngine** | Ó¨ | Keyword & semantic search | 2 | Read-only â€” no transcript mutation |
| **MeetingDetector** | Ó¨ | Real vs hallucinated meetings | 2 | No file modification |
| **LLMProvider** | â˜¤ | Unified local/cloud LLM access | 3 | No domain logic |
| **Analyzer** | â˜¤ | Summarization & outlining | 2 | Delegates all LLM calls |
| **TimestampExtractor** | â˜¾ | Video timestamp OCR via VLM | 2 | Independent of transcription |
| **ProgressWitness** | ğ“‚€ | Pure progress logging | 3 | Zero side effects |

## Compression Results

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Context Window Usage                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TurboScribe full codebase   â”‚  ~109,633 tokens  (438,533 bytes) â”‚
â”‚ SCP full spec (all modules) â”‚   ~5,920 tokens                   â”‚
â”‚ SCP single module prompt    â”‚     ~660 tokens   (avg)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Full-spec compression       â”‚  18Ã—                              â”‚
â”‚ Per-module compression      â”‚  166Ã—                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What this means in practice:**

- The AI sees **~660 tokens** of precise contracts instead of **~110,000 tokens**
  of raw code. This leaves 99.4% of the context window free for reasoning.
- A 4K context window model can implement any single TurboScribe module.
- A 128K context window model never needs to page or truncate â€” it processes
  the entire module spec in one shot with massive room for chain-of-thought.

## How SCP Prevents Regressions

### 1. RAG Denial

When implementing the `Transcriber` module, the AI sees:

- âœ… Transcriber's own contract (methods, constraints, glyph rules)
- âœ… AudioIngest and VoiceDetector **interfaces** (what they return)
- âŒ AudioIngest implementation (how they load files)
- âŒ SearchEngine, MeetingDetector, Analyzer (everything downstream)
- âŒ TimestampExtractor (independent branch)

The AI **cannot** accidentally import `SearchEngine` internals into
`Transcriber` because `SearchEngine` doesn't exist in its context.

### 2. Glyph Contracts

Each method is governed by a Chevron primitive that enforces behavioral rules:

| Glyph | Name | Rule | Violation Example |
|:---:|---|---|---|
| â—¬ | Origin | Appears once per program, no nesting | Calling `load_model()` inside a loop |
| â˜¾ | Fold Time | Must have a reachable base case | Infinite recursion in `batch_transcribe()` |
| Ó¨ | Filter | Pure gate â€” input passes or is rejected | `keyword_search()` modifying transcripts |
| ğ“‚€ | Witness | Observe-only â€” zero side effects | `emit_progress()` raising an exception |
| â˜¤ | Weaver | Braids multiple streams into one | `generate()` returning raw API response |

The AI cannot generate a `Transcriber.transcribe_file()` that also does
search, because the â˜¾ (Fold Time) glyph requires a pure fold operation
with a reachable base case â€” not a multi-purpose function.

### 3. Weaver Verification

After the AI generates code, the Weaver (â˜¤) produces a verification
prompt that checks:

- All declared methods are implemented
- No forbidden imports are present
- Glyph contracts are satisfied
- Dependency boundaries are respected

A passing verification returns `W(G) = 0`, meaning zero coupling violations.

## How This Was Constructed

### Step 1: Analyze the Monolith

The original `fast_engine.py` was analyzed to identify natural boundaries:

```python
# 10 modes found in fast_engine.py:
#   mode 1: transcribe_file   â†’ Transcriber
#   mode 2: batch_transcribe   â†’ Transcriber
#   mode 3: transcribe_dir     â†’ Transcriber
#   mode 4: vad_scan           â†’ VoiceDetector
#   mode 5: batch_vad_scan     â†’ VoiceDetector
#   mode 6: search             â†’ SearchEngine
#   mode 7: semantic_search    â†’ SearchEngine
#   mode 8: run_server         â†’ (excluded â€” HTTP wrapper)
#   mode 9: analyze            â†’ Analyzer
#   mode 10: detect_meetings   â†’ MeetingDetector
```

Shared concerns were extracted:
- Audio loading â†’ `AudioIngest`
- LLM calls â†’ `LLMProvider`
- Progress output â†’ `ProgressWitness`
- `timestamp_engine.py` â†’ `TimestampExtractor`

### Step 2: Assign Glyphs

Each module's primary operation was mapped to a Chevron primitive:

- **Data entry points** (file discovery, model loading) â†’ â—¬ Origin
- **Batch/recursive operations** (transcription, timestamp extraction) â†’ â˜¾ Fold Time
- **Pass/reject gates** (search, VAD, meeting detection) â†’ Ó¨ Filter
- **Pure observers** (progress logging) â†’ ğ“‚€ Witness
- **Multi-stream combiners** (LLM + prompt â†’ response) â†’ â˜¤ Weaver

### Step 3: Define Contracts

For each module, we specified:
1. **Methods** â€” exact signatures with input/output types
2. **Glyph per method** â€” which primitive governs its behavior
3. **Constraints** â€” what the module must NOT do
4. **Dependencies** â€” which other modules' interfaces it may call

### Step 4: Generate & Verify

```bash
# Generate SCP prompt for any module
python examples/turboscribe_example.py Transcriber

# Generate + implement + verify with Gemini
set GEMINI_API_KEY=your-key
python examples/turboscribe_example.py Transcriber --gemini
```

The Gemini output was verified by the Weaver: **PASS: W(G) = 0**.

## Quick Start

```bash
# See the full architecture
python examples/turboscribe_example.py

# Generate a module prompt (copy into any AI chat)
python examples/turboscribe_example.py AudioIngest

# End-to-end with Gemini
set GEMINI_API_KEY=your-key
python examples/turboscribe_example.py Transcriber --gemini
```

## File Structure

```
chevron/
â”œâ”€â”€ scp_bridge.py                    # Core SCP Bridge (NOT modified)
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ turboscribe.chevron          # Architecture in Chevron glyph DSL
â”‚   â”œâ”€â”€ turboscribe_example.py       # Standalone TurboScribe SCP spec + CLI
â”‚   â”œâ”€â”€ TURBOSCRIBE_README.md        # This file
â”‚   â””â”€â”€ gemini_example.py            # Generic Gemini integration example
```

## Why This Matters

Traditional AI code generation asks the model to understand your **entire
codebase** before making a change. This is like asking a surgeon to memorize
every patient in the hospital before performing one operation.

SCP flips this: the AI sees **only the operating table** â€” the one module it
needs to implement, with precise contracts for everything it touches. The
result is code that fits into the existing architecture by construction,
not by coincidence.

For TurboScribe, this means:
- **18Ã— less context** for the full architecture spec
- **166Ã— less context** per individual module
- **Zero coupling violations** verified by the Weaver
- **Any AI model** (even 4K context) can implement a single module correctly
